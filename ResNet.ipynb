{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f6b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  am_analysis import am_analysis as ama\n",
    "import skimage.metrics as metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b48a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a0ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR MODULATION SPECTROGRAM\n",
    "def modSpec(x, fs):\n",
    "    win_size_sec = 0.04  # window length for the STFFT (seconds)\n",
    "    win_shft_sec = 0.01  # shift between consecutive windows (seconds)\n",
    "\n",
    "    stft_modulation_spectrogram = ama.strfft_modulation_spectrogram(\n",
    "        x,\n",
    "        fs,\n",
    "        win_size=round(win_size_sec * fs),\n",
    "        win_shift=round(win_shft_sec * fs))\n",
    "\n",
    "    return stft_modulation_spectrogram\n",
    "\n",
    "def specImage(filename):\n",
    "    fs, x = wavfile.read(filename)\n",
    "    x_name = ['speech']\n",
    "    x = x / np.max(x)\n",
    "    # 1s segment to analyze\n",
    "    # x = x[int(fs*1.6) : int(fs*3.6)]\n",
    "\n",
    "    X_data = modSpec(x, fs)\n",
    "\n",
    "    ama.plot_modulation_spectrogram_data(X_data,\n",
    "                                         0,\n",
    "                                         modf_range=np.array([0, 20]),\n",
    "                                         c_range=np.array([-90, -50]))\n",
    "\n",
    "    # Get the current figure and convert it to a 3D array\n",
    "    fig = plt.gcf()\n",
    "    fig.canvas.draw()\n",
    "    plot_data_rgba = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "    plt.close()  # Close the plot to free up resources\n",
    "\n",
    "    # Remove the alpha channel to get a 3D array\n",
    "    plot_data_rgb = plot_data_rgba[:, :, :3]\n",
    "\n",
    "    return plot_data_rgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9afe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the spectrogram image\n",
    "filepath1 = \"../ResNet/EATD_Corpus_Complete/Test/Test_D/negative_out_84.wav\"\n",
    "img1 = specImage(filepath1)\n",
    "\n",
    "filepath2 = \"../ResNet/EATD_Corpus_Complete/Test/Test_D/negative_out_84.wav\"\n",
    "img2 = specImage(filepath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf0799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssimFromAudio(filepath1, filepath2,win_size=11):\n",
    "    img1 = specImage(filepath1)\n",
    "    img2 = specImage(filepath2)\n",
    "    ssim_score = metrics.structural_similarity(img1, img2, win_size=win_size, channel_axis=2)\n",
    "    return ssim_score\n",
    "def playAudio(path):\n",
    "    return Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f02371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EATD\n",
    "# CODE : 1 if Depressed else 0\n",
    "EATD = {\n",
    "    \"TRAIN_D\": {\"src\": \"../ResNet/EATD_Corpus_Complete/Training/Utterances_D/\", \"Storage\" : [], \"Code\" : 1},\n",
    "    \"TRAIN_ND\": {\"src\": \"../ResNet/EATD_Corpus_Complete/Training/Utterances_ND/\", \"Storage\" :[] ,\"Code\" : 0},\n",
    "    \"TEST_D\": {\"src\": \"../ResNet/EATD_Corpus_Complete/Test/Test_D/\", \"Storage\": [],\"Code\" : 1},\n",
    "    \"TEST_ND\": {\"src\": \"../ResNet/EATD_Corpus_Complete/Test/Test_ND/\", \"Storage\": [],\"Code\" : 0}\n",
    "}\n",
    "for key in EATD.keys():\n",
    "    EATD[key][\"Storage\"] =  glob.glob(EATD[key][\"src\"] + \"*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1671150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 57/57 [00:44<00:00,  1.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 192/192 [02:55<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "EATD_SPEC_TRAIN = []\n",
    "\n",
    "column = 'TRAIN_D'\n",
    "code_value = EATD[column]['Code']  # Get the code value outside the loop\n",
    "\n",
    "for i in tqdm(EATD[column]['Storage']):\n",
    "    try:\n",
    "        EATD_SPEC_TRAIN.append({\"Image\": specImage(i), \"Code\": code_value})\n",
    "    except:\n",
    "        print(f\"Error in {i}\")\n",
    "        \n",
    "column = 'TRAIN_ND'\n",
    "code_value = EATD[column]['Code']  # Get the code value outside the loop\n",
    "\n",
    "for i in tqdm(EATD[column]['Storage']):\n",
    "    try:\n",
    "        EATD_SPEC_TRAIN.append({\"Image\": specImage(i), \"Code\": code_value})\n",
    "    except:\n",
    "        print(f\"Error in {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "473bcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(EATD_SPEC_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriha\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sriha\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] Loss: 0.5171\n",
      "Epoch [1/2] Loss: 0.8449\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define your custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_df, transform=None):\n",
    "        self.data_df = data_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data_df.iloc[idx]['Image']\n",
    "        label = self.data_df.iloc[idx]['Code']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Assuming you have a DataFrame named df with \"Image\" and \"Code\" columns\n",
    "# Modify the preprocessing as needed\n",
    "transform = transforms.Compose([transforms.ToTensor()])  # You can add more transformations here\n",
    "train_dataset = CustomDataset(data_df=df, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the ResNet model\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False)\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Define the model and optimizer\n",
    "num_classes = 2  # Binary classification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetModel(num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_outputs = model(batch_inputs)\n",
    "        loss = criterion(batch_outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"resnet_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Load the trained model\n",
    "model = ResNetModel(num_classes)  # Instantiate your model\n",
    "model.load_state_dict(torch.load(\"resnet_model.pth\"))  # Load the saved model weights\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Create a list to store predicted classes\n",
    "predicted_classes = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the 3D image array from the DataFrame\n",
    "    input_image_array = row[\"Image\"]\n",
    "    \n",
    "    # Preprocess the input image array\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),          # Convert to PIL Image\n",
    "        transforms.Resize((224, 224)),    # Resize the image to the same size used during training\n",
    "        transforms.ToTensor(),            # Convert to tensor\n",
    "        transforms.Normalize(             # Normalize pixel values\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    input_tensor = transform(input_image_array).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    predicted_classes.append(predicted_class)\n",
    "\n",
    "# Add the predicted classes to the DataFrame\n",
    "df[\"Predicted_Class\"] = predicted_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f598161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the DataFrame with predicted classes\n",
    "print(df.drop('Image',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bef6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
